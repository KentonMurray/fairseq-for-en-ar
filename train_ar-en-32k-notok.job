#!/bin/bash
#$ -q gpu.q@@2080
#$ -l mem_free=12G,h_rt=200:00:00,gpu=4
#$ -M hxu64@jhu.edu
#$ -m bea
#$ -cwd

###Code to run
#conda init bash
source ~/.bashrc
#conda activate /home/hltcoe/kmurray/code/pytorch
conda activate fairseq
#conda activate /home/hltcoe/bthompson/anaconda3/envs/bigdata
#source activate /home/hltcoe/bthompson/anaconda3/envs/bigdata

#FAIRSEQ=/home/hltcoe/kmurray/code/fairseq/2020-15-01/
FAIRSEQ=/home/haoranxu/fairseq

module load cuda10.1/toolkit
echo $CUDA_VISIBLE_DEVICES

nvidia-smi
hostname


#TODO: Note that I got rid of share all embeddings
#TODO: Batch size was too big
CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train data_transbpe/en-ar-notok-32k-data-bin/ --tokenizer transformer_tokenizer --arch transformer_wmt_en_de --ddp-backend no_c10d --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1.0 --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --update-freq 10 --attention-dropout 0.1 --activation-dropout 0.1 --max-epoch 40 --save-dir models/transbpe-enar-encoder --encoder-embed-dim 768 --decoder-embed-dim 768 --batch-size 16 #--no-epoch-checkpoints #--distributed-world-size 4
